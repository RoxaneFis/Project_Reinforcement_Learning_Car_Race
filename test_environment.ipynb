{"cells":[{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import numpy\n","import numpy as np\n","import itertools as it \n","import gym\n","import torch\n","import torch.nn.functional as F \n","import torch.nn as nn\n","from torch.autograd import Variable\n","from skimage import color\n","import torch.optim as optim\n","from collections import deque\n","\n",""]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["#copy\n","\n","class ScaleFloatFram(gym.ObservationWrapper):\n","    def observation(self, obs):\n","        return np.array(obs).astype(np.float64)\n","\n","def make_env(env_name):\n","    env = gym.make(env_name)\n","    return env\n","    #return ScaleFloatFram(env)\n",""]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"/Users/roxanefischer/.local/share/virtualenvs/code-i1XZ64Tp/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\nTrack generation: 1226..1537 -> 311-tiles track\n"}],"source":["env_name = \"CarRacing-v0\"\n","env = make_env(env_name)\n","ob =env.reset()"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["#To understand\n","def one_step(i :int):\n","    action = env.action_space.sample()\n","    #action = [0,1,1]\n","    observation, reward, done, info = env.step(action)\n","    if (i%10==0):\n","        print(f\" action : {action}\")\n","        print(f\"reward : {reward}\")\n","        print(f\"done : {done}\")\n","        plt.show()\n","        plt.imshow(observation)\n","        print(\"------------------\")\n","\n",""]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["np.random.RandomState(42)\n","n_episode = 2\n","max_horizon = 100\n","batch_size = 3\n","gamma = 0.9 \n","num_frame_stack = 4\n","epsilon = 0.1\n","\n","#direction frein acceleration\n","all_actions = np.array( [k for k in it.product([-1, 0, 1], [1, 0], [0.2, 0])])\n","nb_actions = len(all_actions)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["class Unflatten(nn.Module):\n","    \"\"\"\n","    An Unflatten module receives an input of shape (N, C*H*W) and reshapes it\n","    to produce an output of shape (N, C, H, W).\n","    \"\"\"\n","    def __init__(self, N=-1, C=3,H=96, W=96):\n","        super(Unflatten, self).__init__()\n","        self.N = N\n","        self.C = C\n","        self.H = H\n","        self.W = W\n","    def forward(self, x):\n","        return x.view(self.N, self.C,self.H, self.W)\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        N, C, H, W = x.size() # read in N, C, H, W\n","        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["class Q_model(nn.Module):\n","    \"\"\"\n","    Build and return a PyTorch model implementing the architecture above.\n","    \"\"\"\n","\n","    def __init__(self, batch_size=1, nb_frames=4, output_dim=nb_actions, trainable=True):\n","        super(Q_model,self).__init__()\n","        self.model = nn.Sequential(\n","                # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","                Unflatten(batch_size, nb_frames, 96, 96),\n","                #Unflatten(1, 3, 96, 96),\n","                nn.Conv2d(in_channels=4, out_channels=16, kernel_size=7, stride=3), #46\n","                nn.LeakyReLU(inplace=True, negative_slope=0.01),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","                nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2), #42\n","                nn.LeakyReLU(inplace=True, negative_slope=0.01),\n","                nn.MaxPool2d(kernel_size=2, stride=2), #21\n","                Flatten(),\n","                nn.Linear(288, 256),\n","                nn.LeakyReLU(inplace=True, negative_slope=0.01),\n","                nn.Linear(256, output_dim),)\n","\n","            #nn.Linear(input_dim,48),\n","            #nn.LeakyReLU(0.01),\n","            #nn.Linear(48,48),\n","            #nn.LeakyReLU(0.01),\n","            #nn.Linear(48,output_dim)\n","            \n","\n","            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    \n","    def forward(self,x):\n","         return self.model(x)"]},{"cell_type":"markdown","execution_count":47,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"can't assign to function call (<ipython-input-47-f302ac683773>, line 1)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-f302ac683773>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    torch.Size([2, 28224]) = 64 * 21 *21\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to function call\n"]}],"source":"torch.Size([2, 28224]) = 64 * 21 *21\n"},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":"\ndef initialize_deque(obs,num_frame_stack=num_frame_stack):\n    d = deque(maxlen=num_frame_stack)\n    for i in range(num_frame_stack):\n        d.appendleft(obs)\n    return d\n\ndef stack_to_vector(deque : deque):\n    array = np.array(deque)\n    return torch.from_numpy(array).float()\n    \ndef to_grey(obs):\n    return color.rgb2gray(obs) \n\ndef find_index(all_actions, action):\n    for i in range(len(all_actions)):\n        if (np.array_equal(all_actions[i],action)):\n            return i\n"},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":"\n#optimizer = optim.Adam(Q.parameters(), lr=0.001, betas=(0.5, 0.999))\nmemory = list()\n\ndef main():\n    for i_episode in range(n_episode):\n        print(f\"Episode nb : {i_episode}\")\n        observation = env.reset()\n        d=initialize_deque(to_grey(observation))\n\n        for i_step in range(max_horizon):\n            stack = stack_to_vector(d)\n            Q_current = Q((stack))\n            #select the best action\n            greedy_ind = np.argmax(Q_current.detach().numpy())\n            action = all_actions[greedy_ind]\n           \n    \n            #execute action\n            obs_next, reward, done, info = env.step(action)\n            d.appendleft(to_grey(obs_next))\n            stack_next = stack_to_vector(d)\n            #store trnasition\n            memory.append({\"st\":stack,\"at\":action,\"rt\":reward,\"st+1\":stack_next})\n            #sample minibatch\n            selection = numpy.random.choice(memory, size=batch_size, replace=True)\n\n            rewards=torch.empty(batch_size)\n            #computed_rewards=torch.empty(batch_size)\n            loss = 0.0\n            optimizer.zero_grad()\n            for m in range(batch_size):\n                stack=selection[m][\"st\"]\n                stack_next = selection[m][\"st+1\"]\n                rewards[m]= selection[m][\"rt\"]\n                if (done == False):\n                    greedy_max = np.max(Q(stack_next).detach().numpy())\n                    rewards[m] =+ gamma*greedy_max\n                loss+= F.mse_loss(Q(stack), rewards[m])\n            #loss= F.mse_loss(computed_rewards, rewards)\n\n            loss.backward()\n            #print(loss)\n            #print()\n\n    \n\n"},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[],"source":["\n","np.random.RandomState(42)\n","n_episode = 20\n","max_horizon = 100\n","batch_size = 10\n","gamma = 0.9 \n","num_frame_stack = 4\n","epsilon = 0.1\n","learning_rate = 0.01\n","\n","#direction frein acceleration\n","all_actions = np.array( [k for k in it.product([-1, 0, 1], [1, 0], [0.2, 0])])\n","nb_actions = len(all_actions)\n",""]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[],"source":["tab_loss = list()\n","class Agent():\n","    def __init__(self):\n","        self.target_network = Q_model(batch_size=1, nb_frames=4, output_dim=nb_actions, trainable=True)\n","        self.estimate_network = Q_model(batch_size=1, nb_frames=4, output_dim=nb_actions, trainable=True)\n","        self.optimizer = optim.Adam(self.estimate_network.parameters(),lr=learning_rate)\n","        self.target_parameters = self.target_network.parameters()\n","        self.estimate_parameters = self.estimate_network.parameters()\n","        self.frame = None\n","\n","\n","        self.target_network.eval()\n","        self.estimate_network.train()\n","\n","    def init_frame(self, obs,num_frame_stack=num_frame_stack):\n","        d = deque(maxlen=num_frame_stack)\n","        for i in range(num_frame_stack):\n","            d.appendleft(to_grey(obs))\n","        return d\n","    \n","    def reinitialisation_episode(self):\n","        observation = env.reset()\n","        self.frame = self.init_frame(observation)\n","\n","\n","    def update_target_network(self, tau = 0.1):\n","        for target_param, local_param in zip(self.target_network.parameters(),self.estimate_network.parameters()):\n","            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n","        #assert(len(self.target_parameters)==len(self.estimate_parameters))\n","       #for param in range(self.target_parameters):\n","                        \n","                        \n","\n","    def take_action(self, eps):\n","        proba = np.random.uniform(0, 1)\n","        #print(proba)\n","        if (proba > eps) :\n","            #print(\"proba >eps \")\n","            with torch.no_grad():\n","                stack = stack_to_vector(self.frame)\n","                Q_current = self.estimate_network((stack))\n","                print(Q_current)\n","                #select the best action\n","                greedy_ind = np.argmax(Q_current.detach().numpy())\n","                action = all_actions[greedy_ind]\n","        else :\n","           # print(\"proba <eps \")\n","            action_ind = np.random.randint(0, nb_actions)\n","            action =  all_actions[action_ind]\n","        return action\n","\n","    def learn_from_action(self, action):\n","        obs_next, reward, done, info = env.step(action)\n","        stack = stack_to_vector(self.frame)\n","        self.frame.appendleft(to_grey(obs_next))\n","        stack_next = stack_to_vector(self.frame)\n","        #store trnasition\n","        memory.append({\"st\":stack,\"at\":action, \"rt\":reward,\"st+1\":stack_next})\n","        #sample minibatch\n","        selection = numpy.random.choice(memory, size=batch_size, replace=True)\n","\n","        loss_function = nn.MSELoss()\n","\n","        # self.target_network.eval()\n","        # self.estimate_network.train()\n","\n","        targets=torch.empty(batch_size)\n","        predicted=torch.empty(batch_size)\n","\n","        for m in range(batch_size):\n","            action = selection[m][\"at\"]\n","            index_action = find_index(all_actions, action)\n","            with torch.no_grad():\n","                targets[m]= selection[m][\"rt\"]\n","                if (done == False):\n","                    stack_next = selection[m][\"st+1\"]\n","                    greedy_max = np.max(self.target_network(stack_next).detach().numpy())\n","                    targets[m] =+ gamma*greedy_max\n","                #loss+= F.mse_loss(Q(stack), rewards[m])\n","            stack = selection[m][\"st\"]\n","            #rint(f\"index {index_action}\")\n","            #print(len(predicted))\n","            #print(f\"output size {len(self.estimate_network(stack)[0])}\")\n","            #print(f\"output size {len(self.target_network(stack))}\")\n","            #print(self.estimate_network(stack))\n","            predicted[m] = self.estimate_network(stack)[0][index_action]\n","        \n","        loss = loss_function(predicted, targets)\n","        tab_loss.append(loss)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        self.update_target_network()\n","\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[],"source":["def run():\n","    agent = Agent()\n","    for i_episode in range(n_episode):\n","        print(f\"ie episode : {i_episode}\")\n","        agent.reinitialisation_episode()\n","        for i_step in range(max_horizon):\n","            action = agent.take_action(epsilon)\n","            agent.learn_from_action(action)\n","\n",""]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"    362.4399, 319.2814, 422.5274, 758.9777, 367.6027]])\ntensor([[404.8871, 325.6587, 417.2652, 295.5250, 526.5930, 533.3425, 400.9325,\n         344.1428, 349.6837, 464.3168, 638.5199, 342.1607]])\ntensor([[503.2777, 294.0301, 408.0669, 227.2854, 577.7451, 563.0921, 399.7888,\n         324.7328, 410.3503, 508.3963, 549.2173, 319.1672]])\ntensor([[585.6409, 282.7857, 404.8493, 232.0749, 592.5704, 569.2462, 423.2130,\n         329.0074, 445.2994, 512.8536, 508.8481, 328.0544]])\ntensor([[654.1788, 284.3618, 399.1524, 273.8943, 572.5597, 534.8186, 461.3517,\n         361.9951, 454.2913, 482.9589, 497.5745, 361.7279]])\ntensor([[710.2445, 312.5267, 391.9928, 321.6194, 518.5422, 478.4089, 501.4791,\n         419.9505, 441.6737, 432.6529, 493.0436, 400.0014]])\ntensor([[675.0049, 358.8483, 382.9566, 368.3653, 470.8911, 429.5160, 534.1558,\n         472.0211, 436.5027, 382.7291, 500.1408, 434.1553]])\ntensor([[645.1819, 421.2862, 381.8449, 413.3491, 431.0291, 393.0270, 561.3418,\n         514.0322, 443.4919, 341.1601, 513.8755, 465.0331]])\ntensor([[578.6627, 546.9224, 421.3112, 481.2968, 442.7473, 414.5027, 565.5197,\n         532.5659, 570.3557, 374.6942, 473.1184, 497.1117]])\ntensor([[537.3685, 566.6681, 439.8925, 467.9464, 508.4867, 470.4597, 540.8411,\n         499.5594, 643.4666, 442.7875, 422.0258, 485.1239]])\ntensor([[501.0452, 544.5210, 459.9264, 445.7464, 566.6567, 519.5533, 509.9037,\n         462.5282, 708.5228, 509.6906, 360.5427, 467.1727]])\ntensor([[454.4431, 522.6537, 460.4060, 420.2021, 557.3033, 482.6241, 499.4549,\n         454.3403, 635.6241, 500.4379, 299.4889, 460.0074]])\ntensor([[409.2654, 487.8978, 450.9529, 396.7664, 517.0380, 406.8574, 497.6585,\n         461.1329, 508.0652, 456.3016, 253.1005, 458.2673]])\ntensor([[372.7404, 450.1822, 438.2238, 374.0335, 466.3234, 321.8170, 495.7960,\n         471.6446, 375.5105, 402.1559, 211.3615, 455.5466]])\ntensor([[344.7306, 425.5150, 433.7177, 373.0441, 444.9266, 288.1798, 494.3659,\n         473.5950, 301.7230, 377.4197, 199.7525, 455.3971]])\ntensor([[324.9014, 398.2033, 444.3019, 370.2202, 469.2119, 317.4446, 471.1775,\n         449.4603, 317.4583, 412.7841, 182.8245, 441.7611]])\ntensor([[321.1371, 381.0107, 457.0200, 391.7119, 479.4225, 353.7073, 454.7585,\n         431.7829, 341.1862, 440.2012, 200.5406, 436.1591]])\ntensor([[342.2177, 391.2347, 485.1849, 514.4891, 481.0305, 462.0751, 438.1587,\n         412.3138, 443.6550, 480.8824, 354.6143, 450.9830]])\ntensor([[361.5006, 401.2593, 497.5440, 568.0659, 475.3419, 510.3087, 428.9803,\n         404.7933, 493.1518, 496.4212, 432.3444, 458.3304]])\ntensor([[388.0743, 410.9991, 505.2110, 615.1967, 449.4552, 532.5435, 426.4326,\n         407.6217, 511.4569, 493.6250, 501.0096, 468.7257]])\ntensor([[430.2295, 426.8258, 492.2411, 632.6179, 404.7724, 533.1641, 418.9608,\n         413.6403, 496.1926, 474.9328, 571.6569, 468.5758]])\ntensor([[477.4534, 409.7755, 463.6466, 518.4565, 409.4679, 507.1341, 395.7049,\n         396.4975, 454.6093, 491.3258, 491.4179, 425.3679]])\ntensor([[514.6738, 390.9372, 434.0373, 388.7953, 450.4719, 494.0607, 372.7099,\n         383.6691, 429.7029, 526.1777, 390.0300, 384.8414]])\ntensor([[518.9392, 386.1415, 420.0632, 342.9125, 466.2654, 481.6064, 365.8712,\n         380.7276, 420.9857, 528.0866, 357.2769, 372.1144]])\ntensor([[520.7597, 388.7061, 409.7517, 321.0872, 468.1830, 462.0029, 367.5187,\n         388.4310, 406.6173, 511.1973, 350.9627, 374.8199]])\ntensor([[519.1685, 402.7213, 397.5805, 309.9987, 454.6133, 429.7830, 375.3243,\n         403.5888, 380.9258, 474.5233, 353.8670, 382.9265]])\ntensor([[512.6294, 448.1310, 375.1066, 345.1451, 405.3409, 363.0173, 410.6332,\n         443.9756, 331.1393, 381.4609, 408.3145, 422.9767]])\ntensor([[507.7054, 466.3322, 376.6005, 358.6755, 414.2271, 377.4565, 416.4122,\n         446.6990, 352.0398, 375.7318, 435.0411, 434.0454]])\ntensor([[499.3386, 479.1953, 384.5271, 373.5405, 432.2219, 405.5167, 418.6403,\n         445.4665, 384.0394, 381.7758, 462.2970, 444.2336]])\ntensor([[467.4376, 497.1732, 409.3447, 406.0301, 474.0996, 475.8873, 419.1680,\n         429.9270, 465.9053, 408.7510, 516.4477, 458.8282]])\ntensor([[441.8454, 490.7663, 419.2242, 412.9561, 486.7112, 501.1077, 417.1447,\n         421.0465, 496.8398, 423.2143, 525.7770, 461.0820]])\ntensor([[415.1608, 475.0681, 424.8762, 411.6535, 485.5628, 507.4166, 417.1145,\n         415.1462, 503.9816, 427.2460, 518.6259, 458.7323]])\ntensor([[390.4331, 460.3577, 426.8089, 408.2790, 481.1534, 506.0814, 417.8911,\n         411.0607, 501.5292, 426.6293, 508.7460, 452.2413]])\ntensor([[374.2088, 450.4965, 427.0820, 407.1348, 465.2000, 489.5314, 421.6669,\n         413.0879, 487.5325, 415.7178, 500.0686, 446.9281]])\ntensor([[364.0466, 440.5887, 423.0760, 404.8380, 428.2507, 447.0451, 429.3290,\n         422.0035, 448.2060, 387.2967, 484.7030, 444.7136]])\ntensor([[356.7997, 430.9873, 421.7025, 400.9144, 400.7983, 419.0363, 433.5990,\n         427.1508, 421.8638, 372.6666, 469.6997, 440.1515]])\ntensor([[356.4646, 420.1168, 421.5110, 394.4333, 380.3476, 397.4980, 434.7053,\n         429.8213, 402.2696, 364.5337, 451.1423, 433.8529]])\ntensor([[361.1964, 410.6324, 418.7094, 392.0626, 360.3757, 377.1079, 436.2178,\n         432.9487, 383.9137, 355.7078, 436.0861, 428.7724]])\ntensor([[367.2351, 400.0202, 418.3125, 386.5895, 352.2845, 370.8691, 431.0538,\n         430.8059, 379.8053, 363.4656, 418.7841, 419.9577]])\ntensor([[375.4011, 388.2180, 422.8790, 376.9226, 364.4772, 388.3488, 420.6661,\n         419.6391, 398.8724, 395.2260, 399.3507, 409.1832]])\ntensor([[385.7451, 379.9525, 428.6734, 370.3369, 380.9719, 409.1045, 410.8450,\n         407.9038, 420.9355, 427.7109, 383.5376, 398.8009]])\ntensor([[399.2341, 374.5259, 435.5649, 365.2561, 408.1801, 439.6315, 400.4229,\n         394.0367, 452.1617, 468.6067, 370.9964, 387.8210]])\ntensor([[409.1694, 373.5634, 432.9309, 362.9206, 428.9434, 459.2887, 393.7325,\n         384.2350, 471.6559, 493.0366, 362.4221, 380.5003]])\ntensor([[414.0387, 376.5429, 424.0441, 364.2196, 426.3923, 446.6211, 394.7006,\n         385.6175, 456.1789, 485.3441, 357.1674, 380.4401]])\ntensor([[417.6490, 381.4751, 412.9095, 372.3157, 417.9308, 429.1865, 398.9776,\n         390.1819, 437.6173, 471.3866, 359.4675, 384.7694]])\ntensor([[412.6461, 396.9290, 385.8739, 392.8535, 372.3291, 363.8271, 415.3862,\n         410.7368, 363.9792, 405.3393, 370.4232, 399.7544]])\ntensor([[410.9229, 401.1829, 376.7047, 401.0261, 356.7161, 341.4159, 418.7352,\n         414.7608, 338.3252, 382.0417, 374.0020, 404.1988]])\ntensor([[409.6974, 400.1031, 369.0921, 398.8301, 350.4667, 326.6182, 418.0815,\n         413.3391, 319.6710, 369.2661, 368.0389, 402.5316]])\ntensor([[411.9077, 391.6984, 367.4613, 392.9511, 365.5461, 340.8602, 410.6287,\n         401.2853, 329.0895, 382.4662, 358.8605, 395.4707]])\ntensor([[412.7299, 381.5109, 369.0391, 385.5578, 394.0496, 372.0580, 399.1564,\n         382.8293, 358.0060, 410.0495, 349.0954, 385.3931]])\nie episode : 19\nTrack generation: 1015..1273 -> 258-tiles track\ntensor([[232.1541, 234.5606, 212.5239, 292.8294, 235.4025, 267.5554, 233.4428,\n         210.3921, 268.4341, 243.1771, 292.3307, 229.9955]])\ntensor([[258.1027, 243.6136, 236.8578, 290.7576, 292.6423, 319.4140, 239.4001,\n         209.5297, 317.5621, 302.2630, 281.8624, 236.2681]])\ntensor([[290.1149, 265.8081, 267.1564, 301.5594, 339.1022, 357.4151, 263.1132,\n         228.1059, 352.3576, 346.5043, 284.6082, 259.2704]])\ntensor([[320.5896, 295.7859, 297.8716, 323.3228, 369.2834, 374.6077, 298.6437,\n         263.2084, 369.4993, 370.9964, 297.8347, 293.2700]])\ntensor([[353.8483, 329.6153, 331.7606, 351.3106, 399.8427, 391.6211, 338.9741,\n         305.4127, 387.6214, 395.4843, 316.7829, 331.9264]])\ntensor([[368.6066, 353.4189, 344.7833, 381.7991, 383.3953, 369.1982, 370.2100,\n         343.4699, 369.7481, 371.0386, 344.5878, 363.1597]])\ntensor([[373.9691, 369.4947, 350.6195, 402.5284, 364.2371, 347.3334, 389.9740,\n         369.0987, 352.4258, 345.5626, 365.2918, 383.6094]])\ntensor([[374.7465, 384.2236, 353.4535, 417.3953, 346.3435, 329.1715, 403.6407,\n         387.9240, 337.5669, 322.1549, 381.7431, 398.4221]])\ntensor([[375.0212, 394.7224, 357.5049, 427.2719, 336.4294, 320.2911, 411.6259,\n         398.1320, 332.7357, 308.2398, 392.4529, 406.9302]])\ntensor([[375.1935, 400.5729, 363.7799, 432.2707, 337.1306, 324.7980, 412.6425,\n         399.0868, 340.9910, 307.7142, 398.3961, 409.4973]])\ntensor([[379.3164, 399.6949, 370.5659, 428.4547, 350.9261, 343.5436, 405.0863,\n         389.8408, 362.6405, 326.0521, 394.3749, 403.9554]])\ntensor([[382.8689, 396.7863, 376.3311, 420.2807, 364.2687, 361.3878, 395.4336,\n         379.3765, 383.9462, 344.9799, 386.2211, 396.2987]])\ntensor([[385.0202, 392.1244, 380.1855, 409.2953, 374.8724, 375.6881, 386.1582,\n         370.1067, 400.1477, 359.5207, 376.8844, 388.6620]])\ntensor([[382.6927, 385.7360, 381.6319, 399.7686, 378.1485, 380.1847, 379.2023,\n         366.2822, 407.3249, 365.7403, 369.0381, 382.5916]])\ntensor([[378.7479, 380.9522, 381.3908, 392.8759, 371.6449, 369.7880, 376.2592,\n         370.2297, 401.9560, 360.2888, 362.7270, 380.1308]])\ntensor([[375.1195, 380.0706, 378.4680, 391.7570, 352.4644, 345.0465, 380.2319,\n         383.2469, 380.8596, 341.9468, 362.7708, 383.6958]])\ntensor([[371.1064, 377.6041, 375.0006, 392.9431, 331.5076, 321.2200, 384.0270,\n         394.8976, 361.0897, 322.7904, 365.6827, 387.1521]])\ntensor([[367.2029, 374.2524, 371.5791, 394.2574, 316.6929, 308.0195, 384.4923,\n         400.7041, 350.4759, 310.6138, 370.2925, 387.6863]])\ntensor([[364.5663, 369.9030, 369.7285, 393.2982, 316.0689, 313.8463, 380.4311,\n         396.8962, 355.9728, 313.0327, 372.9622, 383.7832]])\ntensor([[365.1603, 365.8433, 371.0365, 389.5777, 327.2403, 332.1143, 374.2518,\n         387.9951, 372.9329, 327.9361, 371.8600, 377.9857]])\ntensor([[366.9622, 362.0087, 372.6307, 383.3523, 343.8451, 355.5428, 367.6388,\n         378.2429, 393.8178, 347.7358, 368.4612, 370.7471]])\ntensor([[367.6962, 357.6835, 373.1716, 377.2314, 358.2052, 375.6829, 360.8912,\n         367.0698, 411.8949, 364.6667, 365.2519, 363.4472]])\ntensor([[366.8029, 354.5893, 371.5260, 374.4205, 362.5804, 385.0963, 357.2788,\n         359.2894, 420.2944, 370.8414, 365.9367, 359.0016]])\ntensor([[364.7065, 355.4059, 367.8937, 375.4745, 356.3226, 381.8048, 358.6835,\n         356.6931, 417.0964, 364.8234, 370.4336, 358.2219]])\ntensor([[367.0546, 360.2250, 362.8156, 379.3518, 339.0685, 362.5461, 365.4974,\n         361.4678, 397.2646, 346.1995, 376.6246, 364.1059]])\ntensor([[368.9286, 363.5072, 358.6669, 379.4705, 325.8722, 344.4569, 371.5585,\n         365.5676, 377.4806, 332.2146, 377.1790, 368.6115]])\ntensor([[370.8835, 365.3120, 355.4195, 377.4288, 315.8882, 330.3094, 375.5655,\n         367.7775, 361.7501, 322.8757, 375.7605, 372.0352]])\ntensor([[372.7687, 366.7094, 353.4785, 373.6374, 309.8402, 320.7500, 377.4191,\n         366.9578, 350.3824, 318.9522, 372.1917, 372.9311]])\ntensor([[375.3900, 365.7905, 353.2081, 367.2519, 312.8879, 323.9008, 375.2279,\n         360.4310, 350.4503, 325.7309, 366.1917, 369.9983]])\ntensor([[376.6972, 362.7480, 353.5575, 360.1105, 321.5505, 334.7000, 370.6300,\n         350.1743, 357.2809, 338.1050, 359.4841, 365.0346]])\ntensor([[378.1940, 358.4060, 354.6397, 350.0226, 336.0305, 350.6328, 362.7278,\n         336.7169, 369.3114, 356.4170, 349.9106, 357.8687]])\ntensor([[376.1179, 350.5669, 355.0249, 338.1266, 355.1704, 371.9213, 350.3762,\n         316.4555, 385.9224, 383.7784, 338.8209, 345.9043]])\ntensor([[372.3784, 345.5204, 353.2288, 333.4857, 357.7358, 372.5342, 345.9275,\n         310.8757, 384.2636, 389.1509, 334.1571, 341.5692]])\ntensor([[364.8646, 342.7185, 348.7332, 332.8422, 351.8910, 361.9535, 345.3386,\n         310.7040, 372.9766, 382.6667, 333.2440, 341.3881]])\ntensor([[357.0499, 341.3180, 342.8572, 334.6031, 336.5770, 340.2997, 348.4673,\n         317.4125, 352.0382, 365.5283, 334.3066, 343.4759]])\ntensor([[348.1418, 339.5500, 336.9115, 333.1657, 322.8094, 318.8253, 350.5454,\n         323.8352, 330.9267, 348.7830, 331.5870, 345.1743]])\ntensor([[338.7659, 337.0677, 331.0655, 331.7674, 311.1964, 300.3006, 352.8701,\n         330.0303, 311.9977, 333.4281, 329.2753, 346.9152]])\ntensor([[330.1989, 332.6452, 326.8110, 329.0243, 306.8560, 290.7534, 352.7182,\n         332.2919, 301.7057, 325.3278, 326.1657, 346.5234]])\ntensor([[322.6049, 327.3241, 323.1840, 322.7996, 306.8470, 284.7808, 350.8219,\n         332.1635, 294.6529, 321.4815, 319.0909, 343.1713]])\ntensor([[316.5878, 320.5505, 321.3634, 313.3499, 313.4128, 287.4698, 344.0556,\n         327.8489, 296.4517, 326.2005, 308.0674, 335.9991]])\ntensor([[311.6512, 314.2323, 320.6036, 305.4145, 322.4614, 294.5187, 336.7603,\n         322.2888, 303.6967, 334.5465, 299.4522, 327.7121]])\ntensor([[308.0018, 309.5434, 319.9981, 302.2192, 329.1187, 300.4669, 331.0659,\n         318.2705, 310.2402, 340.7788, 294.8658, 320.6869]])\ntensor([[304.7674, 305.8871, 319.6152, 300.7226, 335.4210, 306.7469, 326.6339,\n         314.9501, 317.4437, 346.5543, 292.9276, 314.4451]])\ntensor([[298.3922, 303.6696, 317.9200, 305.7443, 337.2712, 313.6494, 322.3118,\n         312.5714, 327.3926, 344.9794, 300.1423, 307.5069]])\ntensor([[295.0424, 304.4575, 316.4591, 308.9457, 334.5839, 314.3342, 321.8735,\n         312.5914, 327.9444, 339.7933, 304.5324, 305.5282]])\ntensor([[292.4602, 304.9573, 312.2062, 312.9179, 323.5256, 307.5204, 320.5582,\n         315.0143, 320.0851, 323.7289, 308.9478, 302.9308]])\ntensor([[291.0538, 305.1338, 310.0057, 314.0073, 318.3434, 304.8217, 319.3762,\n         315.4380, 316.4734, 316.2993, 309.9876, 301.4309]])\ntensor([[290.6503, 304.0142, 308.6091, 311.7646, 317.8555, 306.2766, 316.3931,\n         311.8174, 316.8538, 314.1617, 307.9845, 298.0510]])\ntensor([[290.9091, 301.8109, 307.8078, 305.5189, 322.7771, 311.0851, 312.2411,\n         306.4901, 319.5153, 316.6185, 301.5704, 292.8288]])\ntensor([[291.1151, 299.0710, 307.2228, 297.6397, 327.0567, 313.5956, 308.1852,\n         301.7768, 320.0526, 318.3893, 292.0458, 287.9650]])\ntensor([[295.4804, 295.9702, 306.2498, 288.7825, 330.2621, 314.7812, 304.4394,\n         297.7089, 319.2823, 319.8568, 281.8636, 283.4716]])\ntensor([[301.4222, 292.8100, 302.1057, 278.5868, 328.2792, 309.2255, 302.2454,\n         294.5159, 309.5901, 311.7658, 270.1710, 285.6497]])\ntensor([[302.8986, 291.6174, 299.3645, 274.8192, 325.3861, 305.5186, 301.5013,\n         293.0750, 304.1824, 306.8499, 265.8713, 286.9823]])\ntensor([[303.3003, 290.9767, 293.6893, 271.6271, 319.0406, 297.1163, 300.7540,\n         291.7641, 292.4897, 295.4176, 262.0770, 294.0901]])\ntensor([[303.4328, 291.1737, 291.5909, 272.1211, 313.8357, 292.1002, 301.0079,\n         290.6807, 286.4532, 288.7970, 262.3383, 297.7513]])\ntensor([[302.7773, 291.8238, 289.8260, 274.0214, 308.8098, 288.2790, 300.5636,\n         289.2574, 281.9939, 282.5934, 264.8995, 302.8065]])\ntensor([[299.7447, 290.8177, 288.7302, 275.7562, 305.8853, 288.1330, 299.1927,\n         287.2422, 281.6374, 279.1866, 267.5122, 306.7293]])\ntensor([[297.1142, 287.8408, 288.9788, 276.2373, 307.8730, 294.6367, 294.7961,\n         282.4196, 288.8680, 281.7042, 268.9923, 308.5590]])\ntensor([[294.8888, 284.3414, 289.0105, 275.0685, 310.6189, 301.9741, 288.9850,\n         276.4323, 297.1835, 285.8045, 268.6746, 308.2663]])\ntensor([[291.7307, 280.8302, 288.4240, 272.6560, 311.8907, 306.7353, 283.5418,\n         271.3665, 302.3449, 288.4211, 267.0207, 307.1864]])\ntensor([[288.3831, 278.7428, 286.9592, 272.2060, 309.3812, 306.3489, 281.7166,\n         269.7937, 301.3101, 286.2443, 267.4885, 307.8231]])\ntensor([[284.4872, 275.8696, 285.5525, 270.1688, 308.2628, 307.0065, 279.0121,\n         267.0785, 301.1951, 285.8324, 265.9774, 304.5748]])\ntensor([[280.9644, 273.4164, 284.4172, 268.5906, 310.6865, 311.4050, 275.8987,\n         263.1292, 304.4419, 287.9674, 265.8340, 300.0416]])\ntensor([[281.0645, 272.1186, 283.4102, 269.6998, 311.4343, 314.2644, 275.3849,\n         260.7334, 307.0779, 288.2998, 268.1609, 297.0117]])\ntensor([[280.3112, 272.2425, 281.4167, 273.4221, 306.9322, 310.3705, 279.3448,\n         261.7725, 303.5435, 282.8344, 272.7545, 295.7949]])\ntensor([[279.6046, 272.3934, 279.5296, 276.0347, 302.4448, 306.1819, 282.9640,\n         262.8007, 299.6414, 277.7310, 275.8279, 294.6164]])\ntensor([[279.1676, 272.4302, 278.0811, 278.5381, 298.5606, 302.5932, 285.3623,\n         264.3048, 296.6489, 273.8266, 278.4872, 293.4715]])\ntensor([[278.6008, 272.4400, 276.3967, 280.4351, 295.2844, 299.2446, 288.2944,\n         265.4873, 293.1399, 270.2215, 280.4852, 290.9709]])\ntensor([[278.0839, 272.9145, 274.7999, 281.4778, 292.0469, 295.0991, 290.2082,\n         266.6184, 289.0186, 267.0461, 281.3233, 288.0135]])\ntensor([[277.9770, 272.4570, 274.3595, 281.3705, 292.2643, 296.1321, 290.0476,\n         265.4857, 289.6915, 268.8202, 281.0569, 283.4641]])\ntensor([[277.6246, 272.0502, 273.3964, 280.2471, 290.7876, 293.4714, 290.4583,\n         265.2947, 286.4692, 268.9914, 279.3939, 279.1396]])\ntensor([[277.5325, 270.9389, 272.8748, 276.3492, 292.2037, 293.3734, 289.3943,\n         263.4471, 285.6657, 272.5080, 275.2322, 273.4630]])\ntensor([[277.6201, 269.5117, 272.7286, 272.5101, 294.8475, 295.2956, 287.5072,\n         260.7204, 286.9933, 277.2265, 271.2379, 267.6597]])\ntensor([[277.1183, 267.2968, 275.7654, 267.1711, 298.6568, 299.1492, 283.8817,\n         256.9091, 290.5179, 284.6518, 265.7611, 261.3149]])\ntensor([[274.9511, 265.3852, 277.6320, 262.1138, 301.3138, 301.2092, 281.0564,\n         254.0323, 292.1687, 289.8965, 260.2621, 256.1333]])\ntensor([[272.5789, 263.2833, 278.0862, 256.3687, 301.4185, 297.9054, 278.4071,\n         252.8856, 288.8226, 291.7101, 253.5833, 252.0137]])\ntensor([[269.9359, 263.0719, 276.8588, 253.9528, 297.3207, 289.7435, 278.4613,\n         254.6069, 281.4412, 288.4736, 250.6271, 250.2256]])\ntensor([[267.3322, 263.7977, 274.8741, 253.5890, 290.3921, 279.3045, 279.9463,\n         257.9696, 272.2013, 282.4891, 250.1501, 250.1333]])\ntensor([[264.8498, 264.3148, 272.0235, 254.6843, 281.5649, 267.9567, 282.0538,\n         261.9170, 262.1537, 274.9074, 251.1160, 250.8714]])\ntensor([[262.4336, 265.5026, 268.8490, 257.3407, 269.1097, 253.8643, 286.0258,\n         267.5339, 249.5640, 264.1632, 253.6310, 253.1304]])\ntensor([[260.6994, 266.3428, 266.2873, 259.6106, 262.3436, 248.3106, 288.5647,\n         270.0869, 243.9836, 259.3283, 256.3083, 253.7983]])\ntensor([[258.8767, 266.1480, 262.7843, 260.4675, 259.4927, 250.3309, 288.2198,\n         268.9821, 244.9547, 261.1673, 258.7341, 251.2067]])\ntensor([[258.9504, 264.7784, 262.9321, 258.9550, 263.8646, 259.1734, 284.3999,\n         264.6292, 253.6429, 269.0079, 258.0680, 247.5576]])\ntensor([[259.3177, 262.8596, 263.4472, 255.9879, 270.6190, 271.1714, 278.5298,\n         258.8660, 264.9758, 279.1522, 256.1073, 242.9572]])\ntensor([[259.4793, 261.0486, 263.2120, 254.4639, 274.9682, 280.9894, 273.8736,\n         254.4509, 274.0555, 286.3810, 255.5822, 239.8014]])\ntensor([[258.5204, 260.3861, 261.9454, 255.1760, 272.4189, 283.0402, 271.2161,\n         253.7359, 276.0078, 285.0343, 257.1057, 240.1487]])\ntensor([[257.7812, 259.8141, 260.4669, 256.4357, 269.2899, 283.8418, 268.8716,\n         253.4673, 277.2059, 283.1012, 258.8226, 241.5151]])\ntensor([[255.2637, 261.6559, 255.5330, 261.1888, 256.3000, 276.0268, 267.7622,\n         256.4159, 269.6357, 270.4683, 264.2661, 248.3959]])\ntensor([[253.7276, 263.6653, 252.9817, 262.7443, 247.6838, 267.6735, 269.0316,\n         259.1803, 260.4313, 260.6353, 265.4287, 252.9101]])\ntensor([[252.1783, 265.0248, 252.0817, 263.4358, 244.5030, 264.7997, 268.3827,\n         259.5895, 256.5832, 256.6096, 265.5399, 255.5199]])\ntensor([[251.5362, 263.8599, 252.4011, 259.6979, 247.6755, 268.5105, 263.7867,\n         255.2567, 259.0457, 261.0572, 260.1675, 254.3633]])\ntensor([[250.9687, 261.4066, 252.6936, 254.5720, 253.2760, 274.2010, 258.0903,\n         249.5152, 263.3676, 267.4256, 253.6143, 252.1060]])\ntensor([[250.2999, 258.9490, 252.6631, 249.3124, 258.8686, 278.8860, 252.3720,\n         243.9873, 267.0369, 273.1070, 246.9276, 249.8288]])\n"}],"source":["run()"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x148895198>]"},"execution_count":167,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":"plt.plot(tab_loss)\n"},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":"def find_index(all_actions, action):\n    for i in range(len(all_actions)):\n        if (np.array_equal(all_actions[i],action)):\n            return i\n"},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"data":{"text/plain":"0"},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["find_index(all_actions,a)"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"data":{"text/plain":"True"},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":"np.array_equal(all_actions[0],a)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}