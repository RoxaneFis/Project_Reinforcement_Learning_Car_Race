{"cells":[{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[],"source":"import numpy as np\nimport itertools as it \nimport gym\nimport torch\nimport torch.nn.functional as F \nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n"},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[],"source":"class ScaleFloatFram(gym.ObservationWrapper):\n    def observation(self, obs):\n        return np.array(obs).astype(np.float64)\n\ndef make_env(env_name):\n    env = gym.make(env_name)\n    return ScaleFloatFram(env)\n"},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"/Users/roxanefischer/.local/share/virtualenvs/code-i1XZ64Tp/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"}],"source":"env_name = \"CarRacing-v0\"\nenv = make_env(env_name)"},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Track generation: 1267..1598 -> 331-tiles track\nretry to generate track (normal if there are not many of this messages)\nTrack generation: 1125..1410 -> 285-tiles track\n"}],"source":"ob =env.reset()"},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'numpy.float64' object has no attribute 'type'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-156-7a457ab05992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'type'"]}],"source":"ob[0][0][0].typeimport numpu"},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[],"source":"\ndef one_step(i :int):\n    action = env.action_space.sample()\n    #action = [0,1,1]\n    observation, reward, done, info = env.step(action)\n    if (i%500==0):\n        print(f\" action : {action}\")\n        print(f\"reward : {reward}\")\n        print(f\"done : {done}\")\n        plt.show()\n        plt.imshow(observation)\n        print(\"------------------\")\n"},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[],"source":["for i in range(1,1):\n","    one_step(i)"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[],"source":"\n    # Random seed\nnp.random.RandomState(42)\n # Create Environment\nn_episode = 4\nmax_horizon = 1000\neval_steps = 10\nbatch_size = 10\n\n\n\nmemory = list()\ngamma = 0.9 \nbatch_size = 2\nall_actions = np.array( [k for k in it.product([-1, 0, 1], [1, 0], [0.2, 0])])\nnb_actions = len(all_actions)\n\n"},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[{"data":{"text/plain":"(2, 4, 96, 96)"},"execution_count":160,"metadata":{},"output_type":"execute_result"}],"source":"D = list()\ns=ob\nall_actions = np.array( [k for k in it.product([-1, 0, 1], [1, 0], [0.2, 0])])\naction = all_actions[0]\nreward =1\nobservation = ob\n\nD.append({\"st\":s,\"at\":action,\"rt\":reward,\"st+1\":observation})\nD.append({\"st\":s,\"at\":action,\"rt\":reward,\"st+1\":observation})\ntransitions = np.random.choice(D, size=2, replace=False)\nX = np.array([ (transi[\"st\"]) for transi in transitions])\nX = torch.from_numpy(X).float()\n#X.float()\n##X=target.to(device, dtype= torch.float)\ny = np.array([ transi[\"rt\"]for transi in transitions])\n\nnum_frame_stack = 4\npic_size=(96, 96)\n\ndim_input_batch = (batch_size, num_frame_stack) + pic_size\ndim_input_batch\n#dim_input = (None, self.num_frame_stack) + self.pic_size"},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[],"source":"def Q_model(input_dim=96*96*3, output_dim=nb_actions):\n    \"\"\"\n    Build and return a PyTorch model implementing the architecture above.\n    \"\"\"\n    model = nn.Sequential(\n            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n            Unflatten(batch_size, 3, 96, 96),\n            #Unflatten(1, 3, 96, 96),\n            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=7, stride=3), #46\n            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2), #42\n            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n            nn.MaxPool2d(kernel_size=2, stride=2), #21\n            Flatten(),\n            nn.Linear(288, 256),\n            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n            nn.Linear(256, nb_actions),\n\n            #nn.Linear(input_dim,48),\n            #nn.LeakyReLU(0.01),\n            #nn.Linear(48,48),\n            #nn.LeakyReLU(0.01),\n            #nn.Linear(48,output_dim)\n            \n\n            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n    )\n    return model"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"torch.Size([2, 28224]) = 64 * 21 *21\n"},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"torch.Size([2, 96, 96, 3])\ntorch.Size([2, 12])\n"}],"source":"#print(X.shape)\nreal_data = Variable(X)\nprint(real_data.shape)\nQ = Q_model()\nanswer = Q(real_data)\nprint(answer.shape)\n#np.max(answer[0].detach().numpy())"},{"cell_type":"code","execution_count":193,"metadata":{},"outputs":[],"source":"Q = Q_model()\nimport numpy\ndef main():\n    for i_episode in range(n_episode):\n        observation = env.reset()\n        observation =torch.from_numpy(observation).float()\n        for i_step in range(max_horizon):\n            #tochange\n            #print(observation.shape)\n            #Q(Variable(observation)))\n            #np.argmax(Q(observation).detach().numpy())\n            action = env.action_space.sample()\n            \n            observation, reward, done, info = env.step(action)\n            memory.append({\"st\":observation,\"at\":action,\"rt\":reward,\"st+1\":observation})\n            selection = numpy.random.choice(memory, size=batch_size, replace=False)\n            X = np.array([ transi[\"st\"]for transi in selection])\n            X = torch.from_numpy(X).float()\n            y=torch.empty(batch_size)\n            for m in range(batch_size):\n                y[m] = selection[m][\"rt\"]\n                if (done == False):\n                    y[m] += gamma*Q(observation)\n            Q_value = Q(X)\n            print(Q_value)\n            loss = F.mse_loss(Q_value, y)\n            print(loss)\n            loss.backward()        \n\n\n\n    \n\n"},{"cell_type":"code","execution_count":194,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Track generation: 1118..1402 -> 284-tiles track\n"},{"ename":"TypeError","evalue":"view() takes at most 2 arguments (4 given)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-194-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-193-e4357c5e3114>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mQ_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/code-i1XZ64Tp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/code-i1XZ64Tp/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/share/virtualenvs/code-i1XZ64Tp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-143-a370584ff4ea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: view() takes at most 2 arguments (4 given)"]}],"source":["main()"]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[],"source":"(96, 96, 3)\n\nclass Unflatten(nn.Module):\n    \"\"\"\n    An Unflatten module receives an input of shape (N, C*H*W) and reshapes it\n    to produce an output of shape (N, C, H, W).\n    \"\"\"\n    def __init__(self, N=-1, C=3,H=96, W=96):\n        super(Unflatten, self).__init__()\n        self.N = N\n        self.C = C\n        self.H = H\n        self.W = W\n    def forward(self, x):\n        return x.view(self.N, self.C,self.H, self.W)"},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":"class Flatten(nn.Module):\n    def forward(self, x):\n        N, C, H, W = x.size() # read in N, C, H, W\n        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'time' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-3df6a8f24c3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# bring data to the computing device, e.g. GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"]}],"source":"\n        start = time()\n        for batch_idx, (data, target) in enumerate(trainset_loader):\n            # bring data to the computing device, e.g. GPU\n            data, target = data.to(device), target.to(device)\n\n            # forward pass\n            output = model(data)\n            # compute loss: negative log-likelihood\n            loss = F.nll_loss(output, target)\n            \n            # backward pass\n            # clear the gradients of all tensors being optimized.\n            optimizer.zero_grad()\n            # accumulate (i.e. add) the gradients from this forward pass\n            loss.backward()\n            # performs a single optimization step (parameter update)\n            optimizer.step()\n            \n            if iteration % log_interval == 0:\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n                    100. * batch_idx / len(trainset_loader), loss.item()))\n            iteration += 1\n            \n        end = time()"},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"UsageError: Line magic function `%pycache` not found.\n"}],"source":["%pycache\n","def build_dc_classifier():\n","    \"\"\"\n","    Build and return a PyTorch model for the DCGAN discriminator implementing\n","    the architecture above.\n","    \"\"\"\n","    return nn.Sequential(\n","        Unflatten(BATCH_SIZE, 1, 28, 28),\n","        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1),\n","        nn.LeakyReLU(inplace=True, negative_slope=0.01),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1),\n","        nn.LeakyReLU(inplace=True, negative_slope=0.01),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","        Flatten(),\n","        nn.Linear(4*4*64, 4*4*64),\n","        nn.LeakyReLU(inplace=True, negative_slope=0.01),\n","        nn.Linear(4*4*64, 1),\n","    )\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}