{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import deque\n\n# Meta parameters for the RL agent\nalpha = 0.1\ntau = init_tau = 1\ntau_inc = 0.01\ngamma = 0.99\nepsilon = 0.5\nepsilon_decay = 0.999\nverbose = True\n\n# Define types of algorithms\nSARSA = \"SARSA\"\nQ_LEARNING = \"Q_learning\"\nEPSILON_GREEDY = \"epsilon_greedy\"\nSOFTMAX = \"softmax\"\nGREEDY = \"greedy\"\n\n# Choose methods for learning and exploration\nrl_algorithm = SARSA #Q_LEARNING\nexplore_method = EPSILON_GREEDY\n\n# Draw a softmax sample\ndef softmax(q):\n    assert tau >= 0.0\n    q_tilde = q - np.max(q)\n    factors = np.exp(tau * q_tilde)\n    return factors / np.sum(factors)\n\n# Act with softmax\ndef act_with_softmax(s, q):\n    prob_a = softmax(q[s, :])\n    cumsum_a = np.cumsum(prob_a)\n    return np.where(np.random.rand() < cumsum_a)[0][0]\n\n# Act with epsilon greedy\ndef act_with_epsilon_greedy(s, q):\n    a = np.argmax(q[s, :])\n    if np.random.rand() < epsilon:\n        a = np.random.randint(q.shape[1])\n    return a\n\n# Compute SARSA update\ndef sarsa_update(q,s,a,r,s_prime,a_prime):\n    td = r + gamma * q[s_prime,a_prime] - q[s,a]\n    #diffference entre deu states\n    return q[s,a] + alpha * td\n\n# Compute Q-Learning update\ndef q_learning_update(q,s,a,r,s_prime):\n    td = r + gamma * np.max(q[s_prime, :]) - q[s, a]\n    return q[s,a] + alpha * td\n\n# Evaluate a policy on n runs\n#here n =10 episodes\n#h = ma_horizon = 1000 actions\ndef evaluate_policy(q,env,n,h,explore_type):\n    success_rate = 0.0\n    mean_return = 0.0\n\n    for i in range(n):\n        discounted_return = 0.0\n        s = env.reset()\n\n        # reward_list = []\n        # NUM_EPISODES = 1000\n        # for episode_index in range(NUM_EPISODES): state = env.reset()\n        # done = False\n        # #t = 0\n        # while not done:\n            # action = greedy_policy(state, v_array)\n            # state, reward, done, info = env.step(action) #t += 1\n            # reward_list.append(reward)\n        #     #print(\"Episode finished after {} timesteps ; reward = {}\".format(t, rewar\n        # d))\n        # print(sum(reward_list) / NUM_EPISODES)\n        # env.close()\n\n        for step in range(h):\n            if explore_type == GREEDY:\n               # state, reward, done, info = env.step(action) \n                s,r, done, info = env.step(np.argmax(q[s,:]))\n            elif explore_type == EPSILON_GREEDY:\n                s,r, done, info = env.step(act_with_epsilon_greedy(s,q))\n            elif explore_type == SOFTMAX:\n                s,r, done, info = env.step(act_with_softmax(s,q))\n            else:\n                raise ValueError(\"Wrong Explore Method in evaluation:\".format(explore_type))\n\n            discounted_return += np.power(gamma,step) * r\n\n            if done:\n                success_rate += float(r)/n\n                mean_return += float(discounted_return)/n\n                break\n\n    return success_rate, mean_return\n\ndef main():\n\n    global epsilon\n    global tau\n\n    #Choose environment\n    env_name = 'FrozenLake-v0'\n\n    # Random seed\n    np.random.RandomState(42)\n\n    # Create Environment\n    env = gym.make(env_name)\n\n    # Recover State-Action space size\n    n_a = env.action_space.n\n    n_s = env.observation_space.n\n\n    # Experimental setup\n    n_episode = 1000\n    print(\"n_episode \", n_episode)\n    max_horizon = 1000\n    eval_steps = 10\n\n    # Monitoring perfomance\n    window = deque(maxlen=100)\n    last_100 = 0\n\n    greedy_success_rate_monitor = np.zeros([n_episode,1])\n    greedy_discounted_return_monitor = np.zeros([n_episode,1])\n\n    behaviour_success_rate_monitor = np.zeros([n_episode,1])\n    behaviour_discounted_return_monitor = np.zeros([n_episode,1])\n\n    # Init Q-table\n    q_table = np.zeros([n_s, n_a])\n\n    env.reset()\n\n    # Train for n_episode\n    for i_episode in range(n_episode):\n\n        # Reset a cumulative reward for this episode\n        total_return = 0.0\n\n        # Start a new episode and sample the initial state\n        s = env.reset()\n\n        # Select the first action in this episode\n        if explore_method == SOFTMAX:\n            a = act_with_softmax(s, q_table)\n        elif explore_method == EPSILON_GREEDY:\n            a = act_with_epsilon_greedy(s, q_table)\n        else:\n            raise ValueError(\"Wrong Explore Method:\".format(explore_method))\n\n\n        for i_step in range(max_horizon):\n\n            # Act\n            s_prime, r, done, info = env.step(a)\n\n            total_return += np.power(gamma,i_step) *r\n\n            # Select an action\n            if explore_method == SOFTMAX:\n                a_prime = act_with_softmax(s_prime, q_table)\n            elif explore_method == EPSILON_GREEDY:\n                a_prime = act_with_epsilon_greedy(s_prime, q_table)\n            else:\n                raise ValueError(\"Wrong Explore Method:\".format(explore_method))\n\n            # Update a Q value table\n            if rl_algorithm == SARSA:\n                q_table[s, a] = sarsa_update(q_table,s,a,r,s_prime,a_prime)\n            elif rl_algorithm == Q_LEARNING:\n                q_table[s, a] = q_learning_update(q_table,s,a,r,s_prime)\n            else:\n                raise ValueError(\"Wrong RL algorithm:\".format(rl_algorithm))\n\n            # Transition to new state\n            s = s_prime\n            a = a_prime\n\n            if done:\n                window.append(r)\n                last_100 = window.count(1)\n\n                greedy_success_rate_monitor[i_episode-1,0], greedy_discounted_return_monitor[i_episode-1,0]= evaluate_policy(q_table,env,eval_steps,max_horizon,GREEDY)\n                behaviour_success_rate_monitor[i_episode-1,0], behaviour_discounted_return_monitor[i_episode-1,0] = evaluate_policy(q_table,env,eval_steps,max_horizon,explore_method)\n                if verbose:\n                    print(\"Episode: {0}\\t Num_Steps: {1:>4}\\tTotal_Return: {2:>5.2f}\\tFinal_Reward: {3}\\tEpsilon: {4:.3f}\\tSuccess Rate: {5:.3f}\\tLast_100: {6}\".format(i_episode, i_step, total_return, r, epsilon,greedy_success_rate_monitor[i_episode-1,0],last_100))\n                    #print \"Episode: {0}\\t Num_Steps: {1:>4}\\tTotal_Return: {2:>5.2f}\\tTermR: {3}\\ttau: {4:.3f}\".format(i_episode, i_step, total_return, r, tau)\n\n                break\n\n\n        # Schedule for epsilon\n        epsilon = epsilon * epsilon_decay\n        # Schedule for tau\n        tau = init_tau + i_episode * tau_inc\n\n    plt.figure(0)\n    plt.plot(range(0,n_episode,10),greedy_success_rate_monitor[0::10,0])\n    plt.title(\"Greedy policy with {0} and {1}\".format(rl_algorithm,explore_method))\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Success Rate\")\n\n    plt.figure(1)\n    plt.plot(range(0,n_episode,10),behaviour_success_rate_monitor[0::10,0])\n    plt.title(\"Behaviour policy with {0} and {1}\".format(rl_algorithm,explore_method))\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Success Rate\")\n    plt.show()\n\n\n    #Show an episod\n\n    # for i_step in range(max_horizon):\n    #     env.render()\n    #     a = np.argmax(q_table[s, :])\n    #     s, r, done, info = env.step(a)\n    #     total_return += np.power(gamma,i_step) *r\n    #\n    #     if done:\n    #         print \"Episode: {0}\\t Num_Steps: {1:>4}\\tTotal_Return: {2:>5.2f}\\tFinal_Reward: {3}\".format(1, i_step, total_return, r)\n    #         break\n    #\n    # # Show Policy#\n    #\n    # for s in range(n_s):\n    #     actions = ['LEFT','DOWN','RIGHT','UP']\n    #     print(actions[np.argmax(q_table[s, :])])\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"if __name__ == \"__main__\":\n\n    main()\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}